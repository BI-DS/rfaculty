---
title: "5: Matrices and speed"
subtitle: "R for faculty, Fall 2024"
author: "Jonas Moss"
format:
  revealjs:
    error: True
    theme: [default, custom.scss]
    width: 1600
    height: 900
    embed-resources: True
    mainfont: "Verdana"
    min-scale: 0.2
    max-scale: 2
    echo: true
---

# Matrices

Almost all statistics is based on matrices, and `R` has excellent matrix support.

## Constructing matrices

The `diag` function creates a diagonal matrix.

The identity matrix:
```{r}
i <- diag(3)
i
```

But the diagonal can be anything.

```{r}
diag(seq(1,3))
```

## The `matrix` function

::: {.panel-tabset}
### `matrix`
The `matrix(x, nrow, ncol)` function constructs a matrix by repeating `x` enough times (along the columns) to fill up a matrix.
```{r}
matrix(1, nrow = 4, ncol = 4)
```


```{r}
x <- matrix(seq(9), nrow = 3, ncol = 3)
x
```

### Exercise
Construct the matrix $$
\left[\begin{array}{ccc}
1 & 2 & 3\\
4 & 5 & 6\\
7 & 8 & 9
\end{array}\right]$$ using the `matrix` function and `seq`. Use the documentation of `matrix`.

### Solution
```{r}

```

:::

## Transpose

::: {.panel-tabset}

### Transpose
The transpose of a matrix swaps its rows and columns, e.g.,

$$
\left[\begin{array}{ccc}
1 & 2 & 3\\
4 & 5 & 6\\
7 & 8 & 9
\end{array}\right]^{T}=\left[\begin{array}{ccc}
1 & 4 & 7\\
2 & 5 & 8\\
3 & 6 & 9
\end{array}\right]
$$

### Exercise
Find out how to use the `t` function to transpose a matrix in `R`. Construct $$\left[\begin{array}{ccc}
1 & 2 & 3\\
4 & 5 & 6\\
7 & 8 & 9
\end{array}\right]$$
using `matrix`, `seq`, and the transpose.

### Solution
```{r}

```

:::

## Matrix multiplication

Matrix multiplication is done using
```{r}
#| eval: False
x %*% y
```

For instance,
```{r}
x <- matrix(seq(9), ncol = 3)
```

Recall that `diag(3)` is the identity matrix.

```{r}
x %*% diag(3)
```

## Don't use `*`

The operator `*` uses element-wise multiplication (Hadamard product).

Common source of bugs!

```{r}
x * diag(3)
```


## Solving linear equations

::: {.panel-tabset}
### System
Solve the linear system

$$
\left[\begin{array}{ccc}
1 & 1 & 2\\
1 & 2 & 1\\
2 & 1 & 1
\end{array}\right]\left[\begin{array}{c}
x_{1}\\
x_{2}\\
x_{3}
\end{array}\right]=\left[\begin{array}{c}
0\\
1\\
0
\end{array}\right]
$$

```{r}
x <- matrix(c(1,1,2,
              1,2,1,
              2,1,1), nrow=3,byrow=TRUE)
a <- c(0,1,0)
y <- solve(x,a)
y
```

### Exercise
Verify that $y$ solves $xy=a$ using matrix multiplication.

### Solution
```{r}

```

:::

## Matrix inverse

::: {.panel-tabset}
### Inverse
The matrix inverse, if it exists numerically, is found by calling `solve`.

```{r}
solve(x)
```

**Note:** Solving equations using `solve(x) %*% a` is not recommended, since `solve(x,a)` is more numerically stable and faster! Only calculate the inverse using `solve(x)` if you have to.

### Exercise 
Verify that `solve(x, a)` equals `solve(x) %*% a`. 

### Solution
```{r}

```

:::

## Eigenvalue decomposition and PCA

::: {.panel-tabset}
### Eigendecomposition
Positive semi-definite matrices $A$, covariance matrices in particular, admit an orthogonal eigenvalue decomposition on the form 

$$A=VDV^T,$$

where $V$ is orthogonal (i.e., $V^TV=I$, the identity matrix) and $D$ is diagonal. These are used to do principal component analysis and employed in "all" matrix theory.

In practice, you're probably better off using `psych::principal`, or `lavaan` for confirmatory factor analysis.

### Exercise
Define `x <- cor(mtcars[, 1:3])`. Inspect the eigenvalue decomposition of `x` using `eigen`. Is something amiss vis-Ã -vis the description above? Define the matrices `d` and `v` using the output of the `eigen` function call, and verify that `v %*% d %*% t(v)` equals `x`.

### Solution
```{r}

```

:::

## Matrix powers

::: {.panel-tabset}

### Powers

Eigenvalue decompositions allow you take the $n$th power of a matrix:

$$A^{n}=(VDV^{T})^{n}=VDV^{T}VDV^{T}\cdots VDV^{T}=VD^{n}V^{T}$$


### Exercise
Define `x <- cor(mtcars[, 1:3])`. Make a function `powerfun(x, n)` that uses eigenvalue decomposition to compute the $n$th power of `x`. Calculate the 5th power using this function, then compute the matrix using iterated matrix multiplications `%*%`.

### Solution
```{r}

```

:::

## Linear regression

::: {.panel-tabset}

### Normal equations

**This is not how you do linear regression in `R`!**

$$\hat{\beta}=(X^{T}X)^{-1}X^{T}y$$
We define the following variables.
```{r}
x <- as.matrix(subset(mtcars, select = -c(mpg)))
x <- cbind(1, x)
colnames(x) <- c("(Intercept)", colnames(x)[-1])
y <- mtcars$mpg
```

### Exercise 
Calculate $\hat{\beta}$ for `x` and `y` using matrix operations.

### Solution
```{r}

```

:::


## Singular value decomposition

The singular value decomposition states that any matrix $A$ can be written as

$$A=U\Sigma V^T,$$

where $\Sigma$ is diagonal with positive elements and $U,V$ are semi-orthogonal matrices (i.e., $U^TU = V^TV=1$).

::: {.panel-tabset}

### Exercise
Use the `str` function to inspect the `svd(x)`, where `x` was defined in the previous exercise. Is there anything amiss?

### Solution
```{r}

```

:::

## Calculating linear regression with SVD

::: {.panel-tabset}

### SVD solution
No software package calculates regression using the normal equations since they are numerically unstable.

$$\hat{\beta}=V\Sigma^{-1}U^{T}y$$

### Exercise
Calculate the regression coefficients $\hat{\beta}$ using singular value decomposition.

### Solution
```{r}

```

:::

## Linear regression on matrices

* We have calculated linear regression in two ways.
* But no one ever does that, they use the `lm` function in `R`.
* The `lm.fit` function works directly on matrices.
* `R` estimates linear regression using QR decomposition, which is faster than `svd` but less numerically accurate.

::: {.panel-tabset}
### Exercise
Figure out how to use the `lm.fit` function to estimate $\hat{\beta}$. Verify our earlier computations.

### Solution
```{r}

```

:::

## Other functions

`R` is a matrix programming language with built-in support for most common matrix operations:

* `det` for the determinant. This is slow! Don't test for invertibility using it.
* `chol` for the Cholesky decomposition.
* `chol2inv` for the inverse matrix from a Cholesky decomposition.
* `qr` the QR decomposition.
* `upper.tri` upper triangular matrices.
* `lower.tri` lower triangular matrices.
* `MASS::ginv` Moore--Penrose pseudo-inverse.
* `trcrossprod` The product $XX^T$.
* `x*y` The Hadamard product.
* `x %x% y` The Kronecker product.

# Measuring speed

`R` is *really slow*. A goal when using `R` is to spend as little time in `R` as possible - spend your time in `C++` / `C` / `Fortran` / `Rust`!

You can time your functions using `microbenchmark`. This is only suitable for reasonably fast functions. (Hence the `micro`.)

## Microbenchmark
```{r}
#| eval: False
install.packages("microbenchmark")
```

Let's construct a matrix with `10000` elements.
```{r}
x <- matrix(seq(10000), 100, 100)
dim(x)
```

We'll work more with the notation below later on.

```{r}
slow_means <- \(x) {
  means <- rep(0, nrow(x))
  for (i in seq(nrow(x))) means[i] <- mean(x[i,])
  means
}
```


## Running a bencmark
:::{.panel-tabset}

### Benchmark
Let's see how fast our hand-rolled function is:
```{r}
microbenchmark::microbenchmark(rowMeans(x), slow_means(x))
```

The `R` implementation is much, much slower. Not uncommon to see worse factors much than this.


### Exercise
The speed of algorithms often depend on $n$, and in this case we checked a relatively small matrix. Modify the code to work with a matrix with size `2000 x 2000`, then run the same benchmark.

### Solution
```{r}

```

:::

## Speedups can be unexpected

:::{.panel-tabset}

### Exercise
The mean of a vector can be calculated in multiple ways. For instance, we may write `sum(x) / length(x)`. Write a function `slow_mean_2` that calculates the mean in this way. Compare the speed to `slow_mean`.

### Solution
```{r}

```

:::

# Summary

1. `R` supports most common matrix operations.
2. Make sure to use `%*%` when multiplying matrices.
3. Check performance using `microbenchmark`. (Loads of fun I think, but easy to waste time on.)



